= Execution Model Overview

This section describes the execution time semantics that correspond to the materialised and runtime forms of the Task Planning model.

== Phases of Instantiation

As described in the earlier section <<Execution Concepts>>, Work Plan execution traverses three states: `materialised`, `activated` and `terminated`. These are described in the following sub-sections.

=== Materialisation

When a Work Plan is to be executed, it will be _materialised_, which concretely means instantiating and usually persisting it (this might be avoided for very short run Plans). Some user input may be required, e.g. number of repetitions of repeatable sections. Conditional branches whose conditions or events will never be met can also be removed during materialisation. The materialised representation consists of:

* A copy of the definition form of the Plan;
* An instance of the Plan using Materialised model instances, i.e. the `M_XXX` classes shown below, representing the concrete executable Tasks, each with a reference back to a definition Task. 
* For Plans with repeatable sections, the Materialised model contains these sections 'unrolled' into as many instances as required by the Plan activator. In terms of the materialised model described below, some `M_TASK_GROUP` instances will have their `_members_` containing additional `M_XX` instances. Thus, more than one `M_TASK` may point to the same generating `TASK` from the definition.

The structure created will thus mirror the structure of the original definition, with extra copies for repeatable sections and potentially 'dead branches' removed. 

At materialisation time, an instance of `EXECUTION_HISTORY` is also created, proving a root point to accumulate Plan execution event records.

Advance allocations of Tasks to concrete participants may be made in this phase.

=== Activation

A materialised Work Plan is _activated_ when it is ready to be used. Activation starts the timeline clock for the Work Plan; accordingly, execution times in the constituent Task Plans are converted to absolute clock times, or a form that can be related to clock time, e.g. via the use of the materialised Work Plan timeline and calendar.

As part of activation, the _runtime window_ must be determined. This is a moving window corresponding to the section of a materialised Plan that will be actively executing at any moment. The size of the window would typically be at least a working shift (e.g. 8-12 hours), but would more typically be the number of days typically used in the organisation for lookahead planning. The part of Task Plans within a Work Plan that are inside the runtime window are in active execution, while other parts (both prior and after the window) are not currently executing but are typically available for visualisation.

When the runtime window is known, allocations for the window can be managed, as <<Allocation,described below>>. To enable allocation to take place, a communication channel must be established with each potential worker or worker pool. This may consist of any technical means, including email, instant messager, a ward screen application and so on. Allocations may be filled in a 'just in time' fashion where the technical means exists to enable it (e.g. push communications from the server).

Execution starts by commencing the Work Plan's top-level Task Plans, and putting them into their initial wait states.

For each Task Plan, execution of the Plan section within the runtime window consists approximately of:

* Tasks being displayed within a separate or shared application for each performer, along with lifecycle state of each Task;
* Tasks become _available_ as earlier Tasks are completed or cancelled. When a Task is available to be done, the performer has various options, including doing the work, cancelling the Task as not needed, completing the Task, or aborting it, which abandons the entire Plan;
* performers interact with the visual representation to transition the lifecycle state appropriately;
* as the work proceeds, event records are generated and added to the execution history.
* the Plan remains in the `active` state until abandoned or completed.

=== Termination

A Task Plan _terminates_ when the execution path taken through the materialised Group/Task graph completes, either due to finishing, or due to abandonment at an intermediate Task.

The Task Plan as a whole returns a termination status of `success` or `fail`, which may be used to control behaviour if it is part of a hand-off chain in which a context switch follows termination of one Task Plan.

A Work Plan terminates when all member Task Plans have terminated in the above fashion.

An event record is added to the execution history for each Task Plan termination, and for the final Work Plan termination.

=== Allowed Plan Modifications

After materialisation but prior to activation, changes may be made to any part of the materialised plan, corresponding to adjusting repeated sections and removing or adding back in condition branches according to relevancy with respect to newly evaluated subject variables (e.g. discovering that the patient is pregnant may require certain Decision Branches to be added back in).

Once activation has occurred, changes of the same type may still occur, but only _ahead of the runtime window_, since the parts of the Plan within the window, or already executed can't safely or sensibly be altered.

If other kinds of changes are needed, this is generally a sign that the Plan design contains issues, and that the current version if already in use will be abandoned in favour of a newer version.

== Execution Semantics

=== Performer Allocation

Before a materialised Task Plan can be executed, at least the `_principal_performer_` must be allocated, and depending on how long-running the Plan is, and where the current execution point is, some actors designated by `PERFORMABLE_ACTION._other_participations_` for Tasks within the current runtime window (<<Activation,see above>>) need to be allocated as well.

The allocation process consists of:

* _assignment_ of a Plan or Task to a real actor or worker pool;
* a _claim_ by an individual to accept the request. 

Assigning _communicates_ a request to potential worker(s) (e.g. via application screens, personal messages, scheduling systems etc), one of which accepts the request and thereby becomes the allocated worker. This is done by user(s) authenticating to the Plan execution engine via an application and signing up for specific roles and functions within the Plan. Not all performers are needed at any time, only those implicated in some defined part of the Plan to be executed, e.g. for the current day within a multi-day Plan.

At various moments during the execution of a Plan, a performer may leave and be replaced by another performer, e.g. due to worker shift changeover. This requires a de-allocation of the leaving performer from the Plan and the allocation of a new actor.

=== Resource Allocation

[.tbd]
TBD

=== Task Lifecycle

During execution, each Task is represented by an instance of the materialised form of the Task. Each Task in the executing Plan has a lifecycle consisting of various states it may pass through in time (recorded in `M_TASK._lifecycle_state_` in the materialised model below).

=== Task Availability

Following the design principle <<Allocation,described earlier>>, the execution engine executing a Task Plan can determine the _availability_ (i.e., when the transition `planned` => `available` may occur) for any Task or Task Group as follows:

* *control-flow*: preceding Tasks / Groups within the current Task Group reach a terminal lifecycle state;
* *wait state*: any Task or Group wait state has been exited due to the arrival of the relevant events, including timeline-related events;
* *subject preconditions*: subject preconditions attached to the current Task Action are satisfied.

A Task is considered according to this logic even if performer and/or resources have not been allocated.

The workflow application may provide an override capability so that a Task can be performed before it is determined to be available. This would enable a user to perform the Task anyway, causing the lifecycle transition '_override_` from `planned` to `available`. A corresponding `TASK_EVENT_RECORD` is created recording the use of the override

=== Task Group Lifecycle State

Since a Task Plan is a hierarchical structure consisting of one or more Task Groups, a way of rolling up Task state is needed. Once a Task Group has become `available` and been entered, i.e. any wait state or timing (`PLAN_ITEM._wait_spec_`) has been satisfied, a way of computing its _effective lifecycle state_ is required, so that the Task Group can be considered as a unit within its parent for the purpose of determining cntrol flow.

The following algorithm is used to compute the effective lifecycle state of a Materialised Task Group from the set of states of its members (which may recusively may other Materialised Task Groups).

[source, java]
--------
//
// Infer the state of a collection whose members have states in sourceStates.
// The order of if/else evaluation determines the correct result.
//
TaskState inferredState (Set<TaskState> sourceStates) {
    
    if (sourceStates.contains(Abandoned))
        return Abandoned;
    else if (sourceStates.contains(Available))
        return Available;
    else if (sourceStates.contains(Planned))
        return Planned;
    else if (sourceStates.contains(Suspended))
        return Suspended;
    else if (sourceStates.contains(Underway))
        return Underway;
    else if (sourceStates.contains(Completed))
        return Completed;
    else if (sourceStates.contains(Cancelled))
        return Cancelled;
    else
        return Initial;
}
--------

Because a (materialised) Task Group is also the top-level structure of the runtime Task Plan, the inferred state of a Task Plan as a whole is also provided by this algorithm applied to the top Group.

=== Execution Path

One of the consequences of Tasks being transitioned to terminal lifecycle states such as `completed`, `cancelled`, etc within the hierarchical Task Group structure is that an effective lifecycle state has to be computed for Task Group objects at runtime as well, as shown above. Essentially it computes the effective state for a Task Group at runtime as a terminal state if there are only Tasks in terminal states. In other words, completing, cancelling or abandoning all Tasks within a Group causes completion of the Group, and this applies when it is in parallel or seqential execution mode.

If a Task Group has more complex execution rules (`TASK_GROUP._execution_rules_`) such as 'exit on first Task to complete', then its completion state will be affected by this, and will be calculable according to the particular rules defined. Such rules can be understood as a short-hand for cancelling Tasks that are not needed, so that the effective Group lifecycle state can still be computed in the standard way.

Completing a Group will ripple back up the Task Group hierarchy to the point where the completed Group is not the final outstanding Task or Group in the parent.

This model results in an execution path during normal processing that is effectively a traversal of the acyclic graph represented by the Task Group containment structure.

It also determines as a side effect how logical execution path 'jumps' due to the use of `EVENT_ACTION._resume_action_`.

=== Resume Semantics

The `_resume_type_` and `_resume_location_` attributes of the `RESUME_ACTION` class constitute the possibility of an uncontrolled jump or 'goto' within the Task execution structure. If allowed without limitation, it is likely to lead to undecidable situations in Plan execution, and unreliable execution histories. For example, if the execution history shows that some Task Y was performed, then it would normally be assumed that the preceding Task X had also been performed (even if cancelled), and by extension that any wait state such as an Event Branch had been satisifed by the relevant event being received. If however, a jump to Task Y from some Task A on a completely separate path were allowed, no such inference can be made, without appropriate processing rules regarding such jumps.

To create workable rules, the notion of the _execution path_ described above has to be used, i.e. the path traversed so far throught the Group / Task graph to the current point. Because the graph has no cycles, a _most recent common location_ for the execution path actually taken and the designated resume location can always be found. This location may be somewhere back in the current path, including at the start (no real common point), or the current Task (resume location is ahead, not behind).

Making the execution valid according to the Plan while allowing an arbitrary resumption point requires finding a _valid path_ from the most recent common location to the resume location. This can be done if the intermediate steps from the most recent common point and the resume point can be shown to be traversable. There are three situations that can occur at each node along this path:

* normal Tasks with no `_wait_spec_` (i.e. planned or event-based timing): these may be automatically cancelled, meaning 'not done, not needed';
* normal Tasks with a `_wait_spec_`: these can be traversed if the relevant time or other events are known to have already been received;
* conditional Group structures: these can be traversed if the relevant conditions and/or events are known to be true, or to have already been received, respectively.

Whether the intermediate logical conditions or event wait states (including timeline events) up to the resume location are satisfiable can in general only be known at execution time. This means that at design time, no general rule can be used to limit the choice of a resume location. However, the intermediate wait states and conditions can be determined easily enough and shown in a tool to the designer, enabling at least a guess as to viability.

What actually happens at execution time depends on where the resume location is, as follows:

* *forward resume*: the resume location is ahead of the current point on the execution path; getting there just requires the above algorithm of cancellation with condition and event checks;
* *alternate path*: the resume location is on an alternate branch with respect to the current execution path; this may be treated as for the forward resume case;
* *current path*: the resume location is earlier on the current path.

The last possibility implies the need to _retry_ Tasks already performed, which must be in either the `cancelled` or `completed` state. Assuming that the intention of the resume location is to perform (again) the Task or Group at that location, the latter must be put back into the `available` state. This is enabled by the special transitions `_retry_` from `cancelled` to `available` and `_redo_` from or `completed` to `available`.

This doesn't address what should happen at execution time when conditions or wait states at intermediate nodes from the most recent common point to the resume point cannot be met. The simplest approach is that they are manually overridden, as may already be done in normal path processing. This has the effect that such overrides are  at least recorded in the execution history.

=== Persistence

The run-time instance structure may need to be persisted to enable a partial execution of a long-running Task Plan to be recorded and picked up when later tasks become ready. In theory, this could be within the EHR, but it is recommended that either a specific EHR area be used for this, or that run-time state persistence be implemented outside the EHR proper.

[.tbd]
issue-runtime-persistence: if within the EHR, we could create a new 'pointer' on the EHR object that points to 'task runtime state' data or similar. Is this a useful thing to do?
